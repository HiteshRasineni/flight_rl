{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# train_dqn_full.py\n\"\"\"\nEnhanced DQN training script for Flight Landing.\n- Simulates 3 runway types: Dry, Wet, Icy (different drag physics)\n- Easier start (alt=400m, dist=800m)\n- Softer penalties, stronger landing reward\n- Prints only summary every 200 episodes\n- Works in Colab / Jupyter\n\"\"\"\n\nimport argparse\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n# =====================================================\n# 1️⃣ ENVIRONMENT\n# =====================================================\nclass FlightEnv(gym.Env):\n    \"\"\"Simplified Flight Landing Environment with runway condition physics.\"\"\"\n    def __init__(self, start_alt=400.0, start_dist=800.0):\n        super().__init__()\n        self.observation_space = spaces.Box(\n            low=np.array([0, 0, 0, -30, 0], dtype=np.float32),\n            high=np.array([5000, 300, 10000, 30, 1], dtype=np.float32),\n            dtype=np.float32\n        )\n        self.action_space = spaces.Discrete(5)\n        self.start_alt = start_alt\n        self.start_dist = start_dist\n        self.reset()\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n        self.altitude = float(self.start_alt)\n        self.speed = float(160.0 + np.random.uniform(-10, 10))\n        self.distance = float(self.start_dist)\n        self.prev_distance = self.distance\n        self.angle = float(np.random.uniform(-2, 2))\n        # Runway conditions: 0.0=dry, 0.5=wet, 1.0=icy\n        self.runway_condition = float(np.random.choice([0.0, 0.5, 1.0]))\n        self.steps = 0\n        return self._get_obs(), {}\n\n    def step(self, action):\n        self.steps += 1\n\n        # --- Action effects ---\n        if action == 0:  # throttle up\n            self.speed += 6.0\n        elif action == 1:  # throttle down\n            self.speed -= 6.0\n        elif action == 2:  # pitch up\n            self.altitude += 35.0\n            self.angle += 1.5\n        elif action == 3:  # pitch down\n            self.altitude -= 35.0\n            self.angle -= 1.5\n\n        # --- Dynamics ---\n        self.distance -= max(self.speed * 0.3, 1.0)\n        self.altitude -= 8.0\n\n        # --- Runway physics ---\n        if self.runway_condition == 0.0:  # dry\n            drag_factor = 0.6\n        elif self.runway_condition == 0.5:  # wet\n            drag_factor = 0.4\n        else:  # icy\n            drag_factor = 0.25\n\n        self.speed -= drag_factor\n        self.angle = np.clip(self.angle, -30, 30)\n        self.altitude = max(self.altitude, 0.0)\n        self.speed = np.clip(self.speed, 0.0, 300.0)\n\n        # --- Reward shaping ---\n        reward = 0.0\n        reward += (self.prev_distance - self.distance) * 0.02\n        self.prev_distance = self.distance\n        reward -= 0.03\n        reward -= 0.005 * abs(self.altitude - 100)\n        reward -= 0.005 * abs(self.speed - 150)\n        reward -= 0.01 * abs(self.angle)\n\n        if self.distance < 400:\n            reward += 0.8\n        if 0 < self.altitude < 100 and 100 < self.speed < 200:\n            reward += 1.5\n        reward += (self.start_dist - self.distance) / max(1.0, self.start_dist)\n\n        done = False\n        success = False\n        outcome = \"in-flight\"\n\n        # --- Landing / Crash logic ---\n        if self.distance <= 0:\n            if 0 <= self.altitude <= 50 and 100 <= self.speed <= 200 and abs(self.angle) < 10:\n                reward += 200.0\n                success = True\n                outcome = \"successful landing\"\n            else:\n                reward -= 40.0\n                outcome = \"failed landing\"\n            done = True\n\n        if self.altitude <= 0 and self.distance > 0:\n            reward -= 40.0\n            done = True\n            outcome = \"crash before runway\"\n        if self.speed <= 20 and self.altitude > 100:\n            reward -= 40.0\n            done = True\n            outcome = \"stall midair\"\n        if self.steps >= 600:\n            done = True\n            outcome = \"timeout\"\n\n        info = {\"success\": success, \"outcome\": outcome, \"runway_condition\": self.runway_condition}\n        return self._get_obs(), float(reward), bool(done), False, info\n\n    def _get_obs(self):\n        return np.array([\n            self.altitude / 5000.0,\n            self.speed / 300.0,\n            self.distance / 10000.0,\n            (self.angle + 30.0) / 60.0,\n            self.runway_condition\n        ], dtype=np.float32)\n\n\n# =====================================================\n# 2️⃣ REPLAY BUFFER\n# =====================================================\nclass ReplayBuffer:\n    def __init__(self, capacity: int, obs_shape):\n        self.capacity = int(capacity)\n        self.obs_shape = tuple(obs_shape)\n        self.ptr = 0\n        self.size = 0\n        self.states = np.zeros((self.capacity,) + self.obs_shape, dtype=np.float32)\n        self.next_states = np.zeros((self.capacity,) + self.obs_shape, dtype=np.float32)\n        self.actions = np.zeros((self.capacity,), dtype=np.int64)\n        self.rewards = np.zeros((self.capacity,), dtype=np.float32)\n        self.dones = np.zeros((self.capacity,), dtype=np.float32)\n\n    def push(self, state, action, reward, next_state, done):\n        self.states[self.ptr] = state\n        self.next_states[self.ptr] = next_state\n        self.actions[self.ptr] = int(action)\n        self.rewards[self.ptr] = float(reward)\n        self.dones[self.ptr] = 1.0 if done else 0.0\n        self.ptr = (self.ptr + 1) % self.capacity\n        self.size = min(self.size + 1, self.capacity)\n\n    def sample(self, batch_size: int):\n        idxs = np.random.randint(0, self.size, size=batch_size)\n        return dict(\n            states=self.states[idxs],\n            actions=self.actions[idxs],\n            rewards=self.rewards[idxs],\n            next_states=self.next_states[idxs],\n            dones=self.dones[idxs]\n        )\n\n    def __len__(self):\n        return self.size\n\n\n# =====================================================\n# 3️⃣ AGENT\n# =====================================================\nclass QNetwork(nn.Module):\n    def __init__(self, obs_dim: int, n_actions: int):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(obs_dim, 256), nn.ReLU(),\n            nn.Linear(256, 256), nn.ReLU(),\n            nn.Linear(256, n_actions)\n        )\n\n    def forward(self, x): return self.model(x)\n\n\nclass DQNAgent:\n    def __init__(self, obs_dim, n_actions, lr=3e-4, gamma=0.99, device='cpu'):\n        self.device = torch.device(device)\n        self.q_net = QNetwork(obs_dim, n_actions).to(self.device)\n        self.target_q = QNetwork(obs_dim, n_actions).to(self.device)\n        self.target_q.load_state_dict(self.q_net.state_dict())\n        self.opt = optim.Adam(self.q_net.parameters(), lr=lr)\n        self.loss_fn = nn.MSELoss()\n        self.gamma = gamma\n\n    def act(self, obs, epsilon=0.0):\n        if np.random.rand() < epsilon:\n            return np.random.randint(0, self.q_net.model[-1].out_features)\n        with torch.no_grad():\n            t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n            q = self.q_net(t)\n            return int(q.argmax().item())\n\n    def update(self, batch):\n        s = torch.tensor(batch['states'], dtype=torch.float32, device=self.device)\n        ns = torch.tensor(batch['next_states'], dtype=torch.float32, device=self.device)\n        a = torch.tensor(batch['actions'], dtype=torch.int64, device=self.device)\n        r = torch.tensor(batch['rewards'], dtype=torch.float32, device=self.device)\n        d = torch.tensor(batch['dones'], dtype=torch.float32, device=self.device)\n        q = self.q_net(s).gather(1, a.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            nq = self.target_q(ns).max(1)[0]\n            tgt = r + (1 - d) * self.gamma * nq\n        loss = self.loss_fn(q, tgt)\n        self.opt.zero_grad(); loss.backward()\n        nn.utils.clip_grad_norm_(self.q_net.parameters(), 10.0)\n        self.opt.step()\n        return loss.item()\n\n    def sync_target(self): self.target_q.load_state_dict(self.q_net.state_dict())\n\n\n# =====================================================\n# 4️⃣ TRAINING\n# =====================================================\ndef parse_args():\n    p = argparse.ArgumentParser()\n    p.add_argument('--episodes', type=int, default=50000)\n    p.add_argument('--lr', type=float, default=3e-4)\n    p.add_argument('--buffer_size', type=int, default=30000)\n    p.add_argument('--batch_size', type=int, default=128)\n    p.add_argument('--eps_start', type=float, default=1.0)\n    p.add_argument('--eps_end', type=float, default=0.05)\n    p.add_argument('--eps_decay', type=float, default=0.995)\n    p.add_argument('--target_update', type=int, default=1000)\n    p.add_argument('--save_dir', type=str, default='./checkpoints')\n    p.add_argument('--cpu', action='store_true')\n    p.add_argument('--seed', type=int, default=42)\n    return p.parse_args()\n\n\ndef train(args):\n    device = 'cuda' if torch.cuda.is_available() and not args.cpu else 'cpu'\n    torch.manual_seed(args.seed); np.random.seed(args.seed); random.seed(args.seed)\n    env = FlightEnv()\n    obs_dim = env.observation_space.shape[0]\n    n_actions = env.action_space.n\n    agent = DQNAgent(obs_dim, n_actions, lr=args.lr, gamma=0.99, device=device)\n    buffer = ReplayBuffer(args.buffer_size, (obs_dim,))\n    epsilon = args.eps_start\n    os.makedirs(args.save_dir, exist_ok=True)\n\n    outcomes = {\"successful landing\": 0, \"failed landing\": 0,\n                \"crash before runway\": 0, \"stall midair\": 0, \"timeout\": 0}\n    runway_counts = {0.0: 0, 0.5: 0, 1.0: 0}\n    runway_success = {0.0: 0, 0.5: 0, 1.0: 0}\n\n    best_return = -1e9\n    total_steps = 0\n\n    for ep in range(1, args.episodes + 1):\n        obs, _ = env.reset()\n        ep_return = 0.0\n        done = False\n        outcome = None\n        runway = env.runway_condition\n\n        while not done:\n            action = agent.act(obs, epsilon)\n            nobs, rew, done, _, info = env.step(action)\n            buffer.push(obs, action, rew, nobs, done)\n            obs = nobs\n            ep_return += rew\n            total_steps += 1\n            outcome = info[\"outcome\"]\n\n            if len(buffer) > args.batch_size:\n                batch = buffer.sample(args.batch_size)\n                agent.update(batch)\n\n            if total_steps % args.target_update == 0:\n                agent.sync_target()\n\n        epsilon = max(args.eps_end, epsilon * args.eps_decay)\n        outcomes[outcome] = outcomes.get(outcome, 0) + 1\n        runway_counts[runway] += 1\n        if info[\"success\"]:\n            runway_success[runway] += 1\n\n        # Print summary every 200 episodes\n        if ep % 200 == 0:\n            runway_probs = {k: (runway_success[k] / runway_counts[k] if runway_counts[k] > 0 else 0)\n                            for k in runway_counts}\n            print(\"\\n--- Outcome Summary up to Episode\", ep, \"---\")\n            for k, v in outcomes.items():\n                print(f\"{k:<25}: {v}\")\n            print(\"Runway success probabilities:\", runway_probs)\n            print(\"------------------------------------------\\n\")\n\n        if ep_return > best_return:\n            best_return = ep_return\n            torch.save(agent.q_net.state_dict(), os.path.join(args.save_dir, 'best_model.pt'))\n\n    print(\"✅ Training done.\")\n    print(\"Best episode return:\", best_return)\n    print(\"Final outcomes summary:\")\n    for k, v in outcomes.items():\n        print(f\"{k:<25}: {v}\")\n    final_probs = {k: (runway_success[k] / runway_counts[k] if runway_counts[k] > 0 else 0)\n                   for k in runway_counts}\n    print(\"Final runway success probabilities:\", final_probs)\n\n\n# =====================================================\n# 5️⃣ RUN\n# =====================================================\nif __name__ == \"__main__\":\n    import sys\n    sys.argv = [sys.argv[0]]  # fix for Colab\n    args = parse_args()\n    print(\"Starting training with args:\", args)\n    train(args)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-12T05:26:46.124983Z","iopub.execute_input":"2025-11-12T05:26:46.125253Z","iopub.status.idle":"2025-11-12T05:54:46.849825Z","shell.execute_reply.started":"2025-11-12T05:26:46.125233Z","shell.execute_reply":"2025-11-12T05:54:46.848847Z"}},"outputs":[],"execution_count":null}]}